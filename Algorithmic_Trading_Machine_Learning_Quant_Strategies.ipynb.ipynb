{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29bcecc-ff03-4234-82c6-b7db6607e12a",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Trading Strategy\n",
    "\n",
    "* Download/Load SP500 stocks prices data.\n",
    "* Calculate different features and indicators on each stock.\n",
    "* Aggregate on monthly level and filter top 150 most liquid stocks.\n",
    "* Calculate Monthly Returns for different time-horizons.\n",
    "* Download Fama-French Factors and Calculate Rolling Factor Betas.\n",
    "* For each month fit a K-Means Clustering Algorithm to group similar assets based on their features.\n",
    "* For each month select assets based on the cluster and form a portfolio based on Efficient Frontier max sharpe ratio optimization.\n",
    "* Visualize Portfolio returns and compare to SP500 returns.\n",
    "\n",
    "* https://github.com/Luchkata/Algorithmic_Trading_Machine_Learning/blob/main/Algorithmic_Trading_Machine_Learning_Quant_Strategies.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955cc46c-b84f-4686-99ac-fc780f6f32f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[***********           23%%                      ]  114 of 503 completed"
     ]
    }
   ],
   "source": [
    "from statsmodels.regression.rolling import RollingOLS\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "import pandas_ta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "\n",
    "sp500['Symbol'] = sp500['Symbol'].str.replace('.','-')\n",
    "\n",
    "symbols_list = sp500['Symbol'].unique().tolist()\n",
    "\n",
    "end_date = '2023-09-27'\n",
    "\n",
    "start_date = pd.to_datetime(end_date)-pd.DateOffset(365*8)\n",
    "\n",
    "df = yf.download(tickers=symbols_list, \n",
    "                 start=start_date, \n",
    "                 end=end_date).stack()\n",
    "\n",
    "print('df')\n",
    "print(df)\n",
    "print('sp500')\n",
    "print(sp500)\n",
    "df.index.names = ['date', 'ticker']\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607d60cc-bd71-41a9-9b70-4b8999116602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://help.yahoo.com/kb/SLN28256.html\n",
    "# Adj closeは該当するすべての株式分割および配当分配を調整した後の終値です。\n",
    "# データは、証券価格研究センター (CRSP) の標準に準拠し、適切な株式分割および配当乗数を使用して調整されます。\n",
    "\n",
    "# https://breakingdownfinance.com/finance-topics/risk-management/garman-klass-volatility/\n",
    "# https://portfolioslab.com/tools/garman-klass\n",
    "# Garman Klass は、証券の始値、安値、高値、終値を組み込んだボラティリティ推定式です。\n",
    "# 欠点はあるものの、Garman-Klass 推定値は、時間間隔の開始時と終了時の価格だけでなく、日中の価格の極値も考慮するため、基本的な式よりも効果的です。\n",
    "df['garman_klass_vol'] = ((np.log(df['high'])-np.log(df['low']))**2)/2-(2*np.log(2)-1)*((np.log(df['adj close'])-np.log(df['open']))**2)\n",
    "\n",
    "# RSIとは、「Relative Strength Index」の略で、テクニカルチャートのひとつです。日本語に訳すと「相対力指数」になります。買われすぎか、売られすぎかを判断するための指標として利用されています。\n",
    "# RSIは、過去一定期間の上げ幅（前日比）の合計を、同じ期間の上げ幅の合計と下げ幅の合計を足した数字で割って、100を掛けたものです。\n",
    "# いくら値上がり、値下がりしたかはRSIでは判断できません。数値は0～100で表され、一般的に70～80％以上で買われすぎ、20～30％以下で売られすぎと判断されます。\n",
    "df['rsi'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.rsi(close=x, length=20))\n",
    "\n",
    "# ボリンジャーバンドとは、移動平均線とその上下2本ずつの標準偏差からなる線の計5本の線で表わされます。英字表記は「Bollinger bands」となります。\n",
    "# ボリンジャーバンドは統計学を使って作られていて、大まかにいうと、高い確率で＋2σ(標準偏差)と－2σのラインの間で価格は動くだろうという予測をもとに将来の価格の動きを予測するために使います。\n",
    "# なお、統計学上、＋2σと－2σの間に収まる確率は95.45%とされています。\n",
    "df['bb_low'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:,0])\n",
    "df['bb_mid'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:,1])\n",
    "df['bb_high'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:,2])\n",
    "\n",
    "# 「相場の変動率」を解析する指標（計算式）が「ATR（アベレージトゥルーレンジ)」です。 相場の変動が大きい傾向なのか小さい傾向なのかを分析する場合に有効です。 \n",
    "# 「当日高値-当日安値」「当日高値-前日終値」「当日安値-前日終値」の3つのうち最大の値幅(マド明けを含む最大値幅の計測)を当日の「真の値幅(トゥルーレンジ)」と呼び、\n",
    "# この「真の値幅」の移動平均線がATR(アベレージトゥルーレンジ)です。\n",
    "def compute_atr(stock_data):\n",
    "    atr = pandas_ta.atr(high=stock_data['high'],\n",
    "             low=stock_data['low'],\n",
    "             close=stock_data['close'],\n",
    "             length=14)\n",
    "    return atr.sub(atr.mean()).div(atr.std())\n",
    "    \n",
    "# 今回と関係ないけど日本語でATR出したりと似たようなことしている記事あった　　https://qiita.com/__x__/items/ed91e995aec21ac89c8b\n",
    "df['atr'] = df.groupby(level=1, group_keys=False).apply(compute_atr)\n",
    "\n",
    "# MACD（通称マックディー）は、移動平均の発展版で、更に売買シグナルにおいて精度を高くした、トレンド分析の中でも人気のある指標の一つです。\n",
    "# 「移動平均収束拡散」又は「移動平均収束乖離」などとも呼ばれています。トレンド形成時に威力を発揮するため、逆にボックス相場に弱いのが特徴です。\n",
    "# 主にMACDとMACDシグナルという2本のラインの交差を売買のタイミングとして用いられることが多く、初心者の方でもシグナルの発見が容易です。\n",
    "def compute_macd(close):\n",
    "    macd = pandas_ta.macd(close=close, length=20).iloc[:,0]\n",
    "    return macd.sub(macd.mean()).div(macd.std())\n",
    "\n",
    "df['macd'] = df.groupby(level=1, group_keys=False)['adj close'].apply(compute_macd)\n",
    "\n",
    "# １００万株取引されることがわかっているので1e6で割る(?)\n",
    "df['dollar_volume'] = (df['adj close']*df['volume'])/1e6\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d53e297-5a56-4571-9e27-c2a39973eb50",
   "metadata": {},
   "source": [
    "# Aggregate to monthly level and filter top 150 most liquid stocks for each month.\n",
    "\n",
    "トレーニング時間を短縮し、機能や戦略を実験するために、毎日の営業データを月末の頻度に変換します。\n",
    "To reduce training time and experiment with features and strategies, we convert the business-daily data to month-end frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c55190-972f-473f-bcac-3300f63755fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ここから列の並び順が動画と違う？書いてる内容は同じなはずだが・・・\n",
    "# doller volumeは各銘柄の月全体の平均調整後価格を出したいらしい\n",
    "\n",
    "# 別にuniqueにしなくても良さそうだけどしてるみたい。\n",
    "# 全データが日時なため、日時→月次にしたいらしい。\n",
    "# OHLCとは（Open/High/Low/Close）の省略表記で、ローソク足の価格データセット（始値・高値・安値・終値）である。ここに出来高（Volume）を加え、OHLCVで提供されることも多い。\n",
    "# OHLCは要らないらしくテクニカル指標を指定したいらしい。インジケーターは列を指定する必要があるのでインジケータ\n",
    "last_cols = [c for c in df.columns.unique(0) if c not in ['dollar_volume', 'volume', 'open',\n",
    "                                                          'high', 'low', 'close']]\n",
    "\n",
    "# 時系列データを元データより高い頻度または低い頻度で再度サンプリングすることをリサンプリングと呼ぶ。MはMonthly\n",
    "# 今は日時がスタックしているdfをticker単位でunstackして、dollar_volumeとテクニカル指標をそれぞれ月次にresampleして、dollar_volumeは平均を出しつつ、\n",
    "# テクニカル指標はlastで月末の値をシンプルに指定しつつstackし直す\n",
    "# dropnaは欠損値を落とす\n",
    "data = (pd.concat([df.unstack('ticker')['dollar_volume'].resample('M').mean().stack('ticker').to_frame('dollar_volume'),\n",
    "                   df.unstack()[last_cols].resample('M').last().stack('ticker')],\n",
    "                  axis=1)).dropna()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39579729-9cdb-42a6-9cde-0913964a0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 5-year rolling average of dollar volume for each stocks before filtering.\n",
    "# それを下に各月のもっとも流動性の高い１５０種類をピックアップしたいらしい\n",
    "\n",
    "# ローリング平均では、データ・セットの平均が継続的に更新され、その時点までのデータすべてが算入されます。\n",
    "# 例えば、2012 年 3 月の返品数量のローリング平均は、1 月、2 月、3 月の返品数量を加算し、その合計を 3 で除算して計算されます。\n",
    "# locは行名もしくは列名を指定することで特定の値を抽出できます。（行名や列名をラベルと置き換えて頂いても問題ありません。）\n",
    "# ilocはindexを指定することで特定の値を抽出できます。つまり、行、列を番号（数字が０のインデックス）で指定します。\n",
    "\n",
    "data['dollar_volume'] = (data.loc[:, 'dollar_volume'].unstack('ticker').rolling(5*12, min_periods=12).mean().stack())\n",
    "\n",
    "data['dollar_vol_rank'] = (data.groupby('date')['dollar_volume'].rank(ascending=False))\n",
    "\n",
    "data = data[data['dollar_vol_rank']<150].drop(['dollar_volume', 'dollar_vol_rank'], axis=1)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bccc0a-1b06-4888-991e-92a4f3d3f9d3",
   "metadata": {},
   "source": [
    "# 4.calculate monthly returns for different time horizons as features\n",
    "To capture time series dynamics that reflect, for example, momentum patterns, we compute historical returns using the method .pct_change(lag), that is, returns over various monthly periods as identified by lags.\n",
    "例えば、モメンタムパターンを反映する時系列ダイナミクスを捉えるために、ラグで特定される様々な月次期間のリターンを計算する、 \n",
    ".pct_change(lag)という方法で過去のリターンを計算する、すなわち、様々な月次期間のリターンをラグで識別する。\n",
    "↑訳してもわからん\n",
    "\n",
    "時間軸で分けたいろいろなデータを取ることで該当銘柄のモメンタムを見れるらしい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51614222-7276-433b-81bf-7daf3e1a6864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(df):\n",
    "\n",
    "    outlier_cutoff = 0.005\n",
    "\n",
    "    lags = [1, 2, 3, 6, 9, 12]\n",
    "\n",
    "    for lag in lags:\n",
    "\n",
    "        # diff()がA - Bで差分を算出するのに対し、pct_change()は(A - B) / Bで変化率を算出する。\n",
    "        # 3^4 = 3 ** 4 = pow(3,4) その結果は冪 (べき、英: power) と呼ばれる。表現の揺れにより同じ概念は日本語で「累乗」とも表現されている\n",
    "        # 四分位数とは、データを大きさの順に並べて、個数を４等分できる値のことです。森からうさぎを40羽連れてきて、体の大きさ順に並べます。\n",
    "        # うさぎをピッタリ４等分できる線は、10と11番目の間、20と21番目の間、30と31番目の間の線になります。これが四分位数です。\n",
    "        # pipe.clip 外れ値を四分位範囲を用いてクリップする https://ensekitt.hatenablog.com/entry/2018/02/13/200000\n",
    "        df[f'return_{lag}m'] = (df['adj close']\n",
    "                              .pct_change(lag)\n",
    "                              .pipe(lambda x: x.clip(lower=x.quantile(outlier_cutoff),\n",
    "                                                     upper=x.quantile(1-outlier_cutoff)))\n",
    "                              .add(1)\n",
    "                              .pow(1/lag)\n",
    "                              .sub(1))\n",
    "    return df\n",
    "    \n",
    "    \n",
    "data = data.groupby(level=1, group_keys=False).apply(calculate_returns).dropna()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0aa764-c91f-4535-919b-56dc6a91c8cc",
   "metadata": {},
   "source": [
    "# 5. Download Fama-French Factors and Calculate Rolling Factor Betas\n",
    "Fama-French 5ファクターモデルとは、Fama-French 3ファクターモデルに、収益性（Profitability）と投資（Investment）のファクターを追加したモデルであり、下記式であらわされるモデルである。\n",
    "We will introduce the Fama—French data to estimate the exposure of assets to common risk factors using linear regression.\n",
    "\n",
    "The five Fama—French factors, namely market risk, size, value, operating profitability, and investment have been shown empirically to explain asset returns and are commonly used to assess the risk/return profile of portfolios. Hence, it is natural to include past factor exposures as financial features in models.\n",
    "\n",
    "We can access the historical factor returns using the pandas-datareader and estimate historical exposures using the RollingOLS rolling linear regression.\n",
    "本論文では、線形回帰を用いて一般的なリスク要因に対する資産のエクスポージャーを推定するためのFama-Frenchデータを紹介する。\n",
    "市場リスク、規模、バリュー、営業収益性、投資の5つのFama-Frenchファクターは、経験的に資産リターンを説明することが示されており、ポートフォリオのリスク／リターン・プロファイルを評価するために一般的に使用されている。したがって、過去のファクター・エクスポージャーを財務的特徴としてモデルに含めることは自然なことです。\n",
    "pandas-datareader を使って過去のファクターリターンにアクセスし、RollingOLS のローリング線形回帰を使って過去のエクスポージャを推定することができます。\n",
    "\n",
    "SMB (Small Minus Big) is the average return on the nine small stock portfolios minus the average return on the nine big stock portfolios,\n",
    "\n",
    "HML (High Minus Low) is the average return on the two value portfolios minus the average return on the two growth portfolios,\n",
    "\n",
    "RMW (Robust Minus Weak) is the average return on the two robust operating profitability portfolios minus the average return on the two weak operating profitability portfolios,\n",
    "\n",
    "CMA (Conservative Minus Aggressive) is the average return on the two conservative investment portfolios minus the average return on the two aggressive investment portfolios,\n",
    "\n",
    "Rm-Rf, the excess return on the market, value-weight return of all CRSP firms incorporated in the US and listed on the NYSE, AMEX, or NASDAQ that have a CRSP share code of 10 or 11 at the beginning of month t, good shares and price data at the beginning of t, and good return data for t minus the one-month Treasury bill rate (from Ibbotson Associates).\n",
    "\n",
    "SMB（Small Minus Big）は、9つの小型株ポートフォリオの平均リターンから 9 つの大型株ポートフォリオの平均リターンを差し引いたものです、\n",
    "HML（High Minus Low）は、2つのバリュー・ポートフォリオの平均リターンから2つのグロース・ポートフォリオの平均リターンを差し引いたものです、\n",
    "RMW（Robust Minus Weak）は、2つの強固な営業収益性ポートフォリオの平均リターンから2つの脆弱な営業収益性ポートフォリオの平均リターンを差し引いたものです、\n",
    "CMA（Conservative Minus Aggressive）は、2つの保守的投資ポートフォリオの平均リターンから2つの積極的投資ポートフォリオの平均リターンを引いたものである、\n",
    "Rm-Rfは、米国で設立され、NYSE、AMEX、またはNASDAQに上場しているすべてのCRSP企業のうち、t月初にCRSPの株式コードが10または11であり、t月初に良好な株式と価格のデータ、およびtの良好なリターンのデータから1ヶ月物国庫短期証券レートを差し引いた市場、バリューウェイトリターンの超過リターン（Ibbotson Associatesより）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ed445-df6d-4828-ab8a-03eabb6d1597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kenneth French's data library：ケネス・R・フレンチのHPがpandas_datareaderに対応しているっぽい\n",
    "# https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html\n",
    "factor_data = web.DataReader('F-F_Research_Data_5_Factors_2x3',\n",
    "               'famafrench',\n",
    "               start='2010'\n",
    "              )[0].drop('RF', axis=1)\n",
    "factor_data.index = factor_data.index.to_timestamp()\n",
    "\n",
    "factor_data = factor_data.resample('M').last().div(100)\n",
    "factor_data.index.name = 'date'\n",
    "factor_data = factor_data.join(data['return_1m']).sort_index()\n",
    "\n",
    "factor_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8700844-1130-43d5-81f1-78537605ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 属性sizeで全要素数、属性shapeで形状（行数、列数）が取得できる。要素数が少ない銘柄を除外しているっぽい\n",
    "observations = factor_data.groupby(level=1).size()\n",
    "valid_stocks = observations[observations >= 10]\n",
    "# ここで「key1 が A の行だけフィルタリングしたい」という場合、どうすればいいか。→pandas.MultiIndex.get_level_values というメソッドを使うと、Multi-Index の値にアクセスできる。\n",
    "# isin()メソッドは、リスト内の値が別のリストに含まれているかどうかをチェックするために利用します。リストの値をカンマで区切って指定し、リストの値が含まれている場合はTrue、含まれていない場合はFalseを返します。\n",
    "factor_data = factor_data[factor_data.index.get_level_values('ticker').isin(valid_stocks.index)]\n",
    "factor_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6221f01-c168-4818-b442-2854d83080c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最小二乗法 (OLS) は、最もよく知られている回帰分析手法です。これは、すべての空間回帰分析の開始点でもあります。\n",
    "# 理解または予測しようとしている変数またはプロセスのグローバル モデルを作成し、そのプロセスを表す単一の回帰方程式を作成します。\n",
    "# Rolling OLS applies OLS across a fixed windows of observations and then rolls (moves or slides) the window across the data set. \n",
    "# They key parameter is window which determines the number of observations used in each OLS regression. \n",
    "# By default, RollingOLS drops missing values in the window and so will estimate the model using the available data points.\n",
    "betas = (factor_data.groupby(level=1,\n",
    "                            group_keys=False)\n",
    "         .apply(lambda x: RollingOLS(endog=x['return_1m'], \n",
    "                                     exog=sm.add_constant(x.drop('return_1m', axis=1)),\n",
    "                                     window=min(24, x.shape[0]),\n",
    "                                     min_nobs=len(x.columns)+1)\n",
    "         .fit(params_only=True)\n",
    "         .params\n",
    "         .drop('const', axis=1)))\n",
    "\n",
    "betas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eaf3fe-caf1-42fc-8ab5-e84b358c5240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算したbetasをもともとのdataに混ぜたいが、rolling factorは次の月末の結果を計算しておりそのままだと１ヶ月のズレが生じてしまうため、betasを一ヶ月早めてから(shift())混ぜる\n",
    "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "# shift()デフォルトでは下方向に1行ずれる。行数はそのままなので、最後の行のデータは削除される。\n",
    "data = (data.join(betas.groupby('ticker').shift()))\n",
    "\n",
    "# 欠損値は平均値で埋める\n",
    "data.loc[:, factors] = data.groupby('ticker', group_keys=False)[factors].apply(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "data = data.drop('adj close', axis=1)\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca92e3-2778-44dc-8d22-55005d949df4",
   "metadata": {},
   "source": [
    "# 6. For each month fit a K-Means Clustering Algorithm to group similar assets based on their features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703aece-5731-41bf-b4f7-167e53768839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis: rsi 70 stock has stock momentum\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html \n",
    "# If an array is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.\n",
    "# centroid 【名】 《物理》質量中心\n",
    "target_rsi_values = [30, 45, 55, 70]\n",
    "\n",
    "initial_centeroids = np.zeros((len(target_rsi_values), 18))\n",
    "\n",
    "# 6=rsi\n",
    "initial_centeroids[:,6] = target_rsi_values\n",
    "\n",
    "initial_centeroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416a2679-1414-4da0-9fc8-8108f811bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_clusters(df):\n",
    "    df['cluster'] = KMeans(n_clusters=4,\n",
    "                           random_state=0,\n",
    "                           init=initial_centeroids).fit(df).labels_\n",
    "    return df\n",
    "\n",
    "data = data.dropna().groupby('date', group_keys=False).apply(get_clusters)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728de6a2-a10e-4179-94b1-e592ab135b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(data):\n",
    "\n",
    "    cluster_0 = data[data['cluster']==0]\n",
    "    cluster_1 = data[data['cluster']==1]\n",
    "    cluster_2 = data[data['cluster']==2]\n",
    "    cluster_3 = data[data['cluster']==3]\n",
    "\n",
    "    # scatter first column=atr, second column=rsi\n",
    "    # 動画と自分の表の列の並び順が違うので書き換えちゃう\n",
    "    plt.scatter(cluster_0.iloc[:,0] , cluster_0.iloc[:,6] , color = 'red', label='cluster 0')\n",
    "    plt.scatter(cluster_1.iloc[:,0] , cluster_1.iloc[:,6] , color = 'green', label='cluster 1')\n",
    "    plt.scatter(cluster_2.iloc[:,0] , cluster_2.iloc[:,6] , color = 'blue', label='cluster 2')\n",
    "    plt.scatter(cluster_3.iloc[:,0] , cluster_3.iloc[:,6] , color = 'black', label='cluster 3')\n",
    "    # plt.scatter(cluster_0.iloc[:,5] , cluster_0.iloc[:,1] , color = 'red', label='cluster 0')\n",
    "    # plt.scatter(cluster_1.iloc[:,5] , cluster_1.iloc[:,1] , color = 'green', label='cluster 1')\n",
    "    # plt.scatter(cluster_2.iloc[:,5] , cluster_2.iloc[:,1] , color = 'blue', label='cluster 2')\n",
    "    # plt.scatter(cluster_3.iloc[:,5] , cluster_3.iloc[:,1] , color = 'black', label='cluster 3')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8e66ea-e8a2-43b1-88cc-50a3b1fa3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動画と指標の数値がぜんぜん違うので、多分カラム順番が間違っている\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "for i in data.index.get_level_values('date').unique().tolist():\n",
    "\n",
    "    g = data.xs(i,level=0)\n",
    "    plt.title(f'Date {i}')\n",
    "    plot_clusters(g)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c362ca24-28fc-47ec-b380-1af30bf07ed4",
   "metadata": {},
   "source": [
    "# 7. For each month select assets based on the cluster and form a portfolio based on Efficient Frontier max sharpe ratio optimization\n",
    "First we will filter only stocks corresponding to the cluster we choose based on our hypothesis.\n",
    "\n",
    "Momentum is persistent and my idea would be that stocks clustered around RSI 70 centroid should continue to outperform in the following month - thus I would select stocks corresponding to cluster 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f7e10-2e00-4788-8779-2b3deb8ea74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = data[data['cluster']==3].copy()\n",
    "\n",
    "filtered_df = filtered_df.reset_index(level=1)\n",
    "\n",
    "# 日付が月の最終日になっているので+1日して月初に変えたい\n",
    "filtered_df.index = filtered_df.index+pd.DateOffset(1)\n",
    "\n",
    "# 日付と重みをもたせる\n",
    "filtered_df = filtered_df.reset_index().set_index(['date', 'ticker'])\n",
    "\n",
    "dates = filtered_df.index.get_level_values('date').unique().tolist()\n",
    "\n",
    "fixed_dates = {}\n",
    "\n",
    "for d in dates:\n",
    "    \n",
    "    fixed_dates[d.strftime('%Y-%m-%d')] = filtered_df.xs(d, level=0).index.tolist()\n",
    "\n",
    "# 月ごとに投資するべき銘柄のリストを出力できる\n",
    "fixed_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fa0beb-1678-4bb7-98c8-3b18c8c0cbd2",
   "metadata": {},
   "source": [
    "## Define portfolio optimization function\n",
    "We will define a function which optimizes portfolio weights using PyPortfolioOpt package and EfficientFrontier optimizer to maximize the sharpe ratio.\n",
    "\n",
    "To optimize the weights of a given portfolio we would need to supply last 1 year prices to the function.\n",
    "\n",
    "Apply signle stock weight bounds constraint for diversification (minimum half of equaly weight and maximum 10% of portfolio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd42bc1b-5c24-4a80-abed-c19be651d33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# それぞれの銘柄にどれくらいの割合で投資すべきか重み付けをしたい\n",
    "# 効率的フロンティア　（Efficient Frontier）とは、分散投資を実施したときに実現するポートフォリオの中で、あるリスクの水準で最大のリターンを獲得できるポートフォリオの集合のことを指す。\n",
    "\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "\n",
    "def optimize_weights(prices, lower_bound=0):\n",
    "    \n",
    "    returns = expected_returns.mean_historical_return(prices=prices,\n",
    "                                                      frequency=252)\n",
    "\n",
    "    # sample_covは標本分散共分散行列です。\n",
    "    # 分散共分散行列とは，分散（散らばり具合を表す指標）の概念を多次元確率変数に拡張して行列としたもの。データの散らばり具合や相関という情報を集約したものです！\n",
    "    cov = risk_models.sample_cov(prices=prices,\n",
    "                                 frequency=252)\n",
    "\n",
    "    # solver (string) – which SCIPY solver to use, e.g “SLSQP”, “COBYLA”, “BFGS”. User beware: different optimizers require different inputs.\n",
    "    # シンプレックス法による制約ソルバー（本章では， SCS(simplex constraint solver) と呼ぶ）.\n",
    "    # ソルバーについて(エクセルの記事だけど・・・)\n",
    "    # エクセルのソルバー機能を使用して線形計画問題(Linear Problem)を解く方法について説明します。\n",
    "    # 線形計画問題は、限られたリソースを最適に配分し、行動を最適化するための強力なツールです。そして、エクセルのソルバー機能を使えば、これらの問題を簡単に解くことができます。\n",
    "    ef = EfficientFrontier(expected_returns=returns,\n",
    "                           cov_matrix=cov,\n",
    "                           weight_bounds=(lower_bound, .1),\n",
    "                           solver='SCS')\n",
    "    \n",
    "    weights = ef.max_sharpe()\n",
    "    \n",
    "    return ef.clean_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde62ee5-e80b-40f5-bd64-bfaf5de912de",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = data.index.get_level_values('ticker').unique().tolist()\n",
    "\n",
    "new_df = yf.download(tickers=stocks,\n",
    "                     start=data.index.get_level_values('date').unique()[0]-pd.DateOffset(months=12),\n",
    "                     end=data.index.get_level_values('date').unique()[-1])\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e2b30-e16e-4f57-814a-3fc907c8825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_dataframe = np.log(new_df['Adj Close']).diff()\n",
    "portfolio_df = pd.DataFrame()\n",
    "\n",
    "for start_date in fixed_dates.keys():\n",
    "\n",
    "    try:\n",
    "    \n",
    "        end_date = (pd.to_datetime(start_date)+pd.offsets.MonthEnd(0)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        cols = fixed_dates[start_date]\n",
    "    \n",
    "        optimization_start_date = (pd.to_datetime(start_date)-pd.DateOffset(months=12)).strftime('%Y-%m-%d')\n",
    "        optimization_end_date = (pd.to_datetime(start_date)-pd.DateOffset(days=1)).strftime('%Y-%m-%d') \n",
    "\n",
    "        optimization_df = new_df[optimization_start_date:optimization_end_date]['Adj Close'][cols]\n",
    "\n",
    "        success = False\n",
    "\n",
    "        try:\n",
    "    \n",
    "            weights = optimize_weights(prices=optimization_df,\n",
    "                                   lower_bound=round(1/(len(optimization_df.columns)*2),3))\n",
    "        \n",
    "            weights = pd.DataFrame(weights, index=pd.Series(0))\n",
    "\n",
    "            success = True\n",
    "        except:\n",
    "            print('Max Sharpe Optimization failed for {start_date}, Continuing with Equal-Weights')\n",
    "                   \n",
    "        if success==False:\n",
    "            weights = pd.DataFrame([1/len(optimization_df.columns) for i in range(len(optimization_df.columns))],\n",
    "                                     index=optimization_df.columns.tolist(),\n",
    "                                     columns=pd.Series(0)).T\n",
    "        \n",
    "        temp_df = returns_dataframe[start_date:end_date]\n",
    "\n",
    "        # [Date, Index]だとエラーになるので、[Date, Ticker]にした\n",
    "        temp_df = temp_df.stack().to_frame('return').reset_index(level=0)\\\n",
    "                   .merge(weights.stack().to_frame('weight').reset_index(level=0, drop=True),\n",
    "                          left_index=True,\n",
    "                          right_index=True)\\\n",
    "                   .reset_index().set_index(['Date', 'Ticker']).unstack().stack()\n",
    "        \n",
    "        temp_df.index.names = ['date', 'ticker']\n",
    "\n",
    "        temp_df['weighted_return'] = temp_df['return']*temp_df['weight']\n",
    "\n",
    "        temp_df = temp_df.groupby(level=0)['weighted_return'].sum().to_frame('Strategy Return')\n",
    "\n",
    "        portfolio_df = pd.concat([portfolio_df, temp_df], axis=0)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "portfolio_df = portfolio_df.drop_duplicates()\n",
    "\n",
    "portfolio_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa554328-ebad-4f9c-b2d9-cdde87ea98e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # len(optimization_df.columns)が動画では１０になるのに自分でやると５３になるのはなぜ\n",
    "# weights = optimize_weights(prices=optimization_df,\n",
    "#                            lower_bound=round(1/(len(optimization_df.columns)*2),3))\n",
    "# weights = pd.DataFrame(weights, index=pd.Series(0))\n",
    "\n",
    "# エラーになる\n",
    "# temp_df = returns_dataframe['2017-11-01':'2017-11-30']\n",
    "\n",
    "# temp_df.stack().to_frame('return').reset_index(level=0)\\\n",
    "#     .merge(weights.stack().to_frame('weight').reset_index(level=0, drop=True),\n",
    "#           left_index=True,\n",
    "#           right_index=True)\\\n",
    "#     .reset_index().set_index(['Date', 'index']).unstack().stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b11c8-6b76-4908-a644-3d0b2088312c",
   "metadata": {},
   "source": [
    "# 8. Visualize Portfolio returns and compare to SP500 returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c91b3-d98b-49a6-a66d-65cb4bd23e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy = yf.download(tickers='SPY',\n",
    "                 start='2015-01-01',\n",
    "                 end=dt.date.today())\n",
    "spy_ret = np.log(spy[['Adj Close']]).diff().dropna().rename({'Adj Close':'SPY BuyHold'}, axis=1)\n",
    "\n",
    "portfolio_df = portfolio_df.merge(spy_ret,\n",
    "                                 left_index=True,\n",
    "                                 right_index=True)\n",
    "\n",
    "portfolio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ddfb13-d043-41f9-a01b-8045364a346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 負けとるやん\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "portfolio_cumulative_return = np.exp(np.log1p(portfolio_df).cumsum())-1\n",
    "\n",
    "portfolio_cumulative_return[:'2023-09-29'].plot(figsize=(16,6))\n",
    "\n",
    "plt.title('Unsupervised Learning Trading Strategy Returns Over Time')\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "\n",
    "plt.ylabel('Return')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f984885-2356-425b-8799-edf77ea1fdd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
